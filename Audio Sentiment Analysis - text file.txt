Packages and Libraries
#GENERAL
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import random

#PATH PROCESS
import os
import os.path
from pathlib import Path
import glob
from PIL import Image
!pip install keras tensorflow
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
!pip install opencv-python
import cv2
from keras.applications.vgg16 import preprocess_input, decode_predictions
import imageio
from IPython.display import Image
import matplotlib.image as mpimg

#MUSIC PROCESS
!pip install pydub
import pydub
from scipy.io.wavfile import read, write
!pip install librosa
import librosa
import librosa.display
import IPython
from IPython.display import Audio
import scipy
import pickle

#SCALER & TRANSFORMATION
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tensorflow.keras import regularizers
from sklearn.preprocessing import LabelEncoder

#ACCURACY CONTROL
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
#OPTIMIZER
from tensorflow.keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD

#MODEL LAYERS
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\
                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\
LSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape,\
Conv2DTranspose, LeakyReLU, Conv1D, AveragePooling1D, MaxPooling1D
from keras import models
from keras import layers
import tensorflow as tf
from keras.applications import VGG16,VGG19,inception_v3
from keras import backend as K
from keras.utils import plot_model
from keras.datasets import mnist
import keras

#SKLEARN CLASSIFIER
!pip install xgboost
from xgboost import XGBClassifier, XGBRegressor
!pip install lightgbm
from lightgbm import LGBMClassifier, LGBMRegressor
!pip install catboost
from catboost import CatBoostClassifier, CatBoostRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.cross_decomposition import PLSRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import RidgeCV
from sklearn.linear_model import Lasso
from sklearn.linear_model import LassoCV
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import ElasticNetCV

#IGNORING WARNINGS
from warnings import filterwarnings
filterwarnings("ignore",category=DeprecationWarning)
filterwarnings("ignore", category=FutureWarning) 
filterwarnings("ignore", category=UserWarning)


Path,Label, Transformation
Main
Main_WAV_Path = Path("C:/Users/esther/OneDrive - Tunku Abdul Rahman University College/FYP/Project II/TESS Toronto emotional speech set data")

Wav Path
Wav_Path = list(Main_WAV_Path.glob(r"**/*.wav"))
def extract_emotion(path_string):
    if not isinstance(path_string, str):
        path_string = str(path_string)

    parts = path_string.split(os.sep)

    if len(parts) >= 2:
        potential_emotion_dir = parts[-2] 
    
        if '_' in potential_emotion_dir:
            emotion = potential_emotion_dir.split('_')[1]
            return emotion
        else:
            potential_emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
            if potential_emotion_dir.lower() in potential_emotions:
                emotion = potential_emotion_dir
            else:
                emotion = "unknown"
    else:
        emotion = "unknown"
    return emotion.lower()

Wav Labels
#Wav_Labels = list(map(extract_emotion,Wav_Path))
Wav_Labels = [extract_emotion(path) for path in Wav_Path]

To Series
Wav_Path_Series = pd.Series(Wav_Path,name="WAV").astype(str)
Wav_Labels_Series = pd.Series(Wav_Labels,name="EMOTION")

To Dataframe
Main_Wav_Data = pd.concat([Wav_Path_Series,Wav_Labels_Series],axis=1)
print("\nDataFrame head:")
print(Main_Wav_Data.head())
print("\nEmotion counts:")
print(Main_Wav_Data["EMOTION"].value_counts())

def standardize_existing_dataframe(df, column_name="EMOTION"):
    # Convert all values in the specified column to lowercase
    df[column_name] = df[column_name].str.lower()
    return df

Main_Wav_Data = standardize_existing_dataframe(Main_Wav_Data)

print("\nEmotion counts after standardization:")
print(Main_Wav_Data["EMOTION"].value_counts())

To Shuffle
Main_Wav_Data = Main_Wav_Data.sample(frac=1).reset_index(drop=True)
print("\nShuffled DataFrame head:")
print(Main_Wav_Data.head())


Data Processing Functions
Noise
def add_noise(data):
    noise_value = 0.015 * np.random.uniform() * np.amax(data)
    data = data + noise_value * np.random.normal(size=data.shape[0])
    
    return data

Stretch
def stretch_process(data,rate=0.8):
    
    return librosa.effects.time_stretch(y=data,rate=rate)

Shift
def shift_process(data):
    shift_range = int(np.random.uniform(low=-5,high=5) * 1000)
    
    return np.roll(data,shift_range)

Pitch
def pitch_process(data,sampling_rate,pitch_factor=0.7):
    
    return librosa.effects.pitch_shift(y=data,sr=sampling_rate,n_steps=pitch_factor)

Extract Features
def extract_process(data,sample_rate):
    output_result = np.array([])
    
    mean_zero = np.mean(librosa.feature.zero_crossing_rate(y=data).T,axis=0)
    output_result = np.hstack((output_result,mean_zero))
    
    stft_out = np.abs(librosa.stft(data))
    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft_out,sr=sample_rate).T,axis=0)
    output_result = np.hstack((output_result,chroma_stft))
    
    mfcc_out = np.mean(librosa.feature.mfcc(y=data,sr=sample_rate).T,axis=0)
    output_result = np.hstack((output_result,mfcc_out))
    
    root_mean_out = np.mean(librosa.feature.rms(y=data).T,axis=0)
    output_result = np.hstack((output_result,root_mean_out))
    
    mel_spectogram = np.mean(librosa.feature.melspectrogram(y=data,sr=sample_rate).T,axis=0)
    output_result = np.hstack([output_result, mel_spectogram])

    # if len(output_result) > 704:
        #output_result = output_result[:704]  # Trim excess features
    #elif len(output_result) < 704:
       # output_result = np.pad(output_result, (0, 704 - len(output_result)), mode='constant')
    
    return output_result

Export Features
def export_process(path):
    
    data,sample_rate = librosa.load(path,duration=2.5,offset=0.6)
    
    output_1 = extract_process(data,sample_rate)
    result = np.array(output_1)
    
    noise_out = add_noise(data)
    output_2 = extract_process(noise_out,sample_rate)
    result = np.vstack((result,output_2))
    
    new_out = stretch_process(data)
    strectch_pitch = pitch_process(new_out,sample_rate)
    output_3 = extract_process(strectch_pitch,sample_rate)
    result = np.vstack((result,output_3))
    
    return result


Analysis
Hearing
rate,speech = read(Main_Wav_Data["WAV"][2342])
print(Main_Wav_Data["EMOTION"][2342])

Audio(speech,rate=rate,autoplay=False)

rate,speech = read(Main_Wav_Data["WAV"][3])
print(Main_Wav_Data["EMOTION"][3])

Audio(speech,rate=rate,autoplay=False)

rate,speech = read(Main_Wav_Data["WAV"][2795])
print(Main_Wav_Data["EMOTION"][2795])

Audio(speech,rate=rate,autoplay=False)

Shape - Type
rate,speech = read(Main_Wav_Data["WAV"][2795])
print(Main_Wav_Data["EMOTION"][2795])
print(speech.shape)
print(speech.dtype)
print(rate)

rate,speech = read(Main_Wav_Data["WAV"][314])
print(Main_Wav_Data["EMOTION"][314])
print(speech.shape)
print(speech.dtype)
print(rate)

rate,speech = read(Main_Wav_Data["WAV"][134])
print(Main_Wav_Data["EMOTION"][134])
print(speech.shape)
print(speech.dtype)
print(rate)

Waveplot
figure = plt.figure(figsize=(14,5))

audio_speech,rate = librosa.load(Main_Wav_Data["WAV"][134])
librosa.display.waveshow(audio_speech,sr=rate)
Audio(audio_speech,rate=rate)

figure = plt.figure(figsize=(14,5))

audio_speech,rate = librosa.load(Main_Wav_Data["WAV"][34])
librosa.display.waveshow(audio_speech,sr=rate)
Audio(audio_speech,rate=rate)

figure = plt.figure(figsize=(14,5))

audio_speech,rate = librosa.load(Main_Wav_Data["WAV"][4])
librosa.display.waveshow(audio_speech,sr=rate)
Audio(audio_speech,rate=rate)

Specshow
figure = plt.figure(figsize=(14,5))

audio_speech,rate = librosa.load(Main_Wav_Data["WAV"][458])

stft_audio = librosa.stft(audio_speech)
Db_audio = librosa.amplitude_to_db(abs(stft_audio))
librosa.display.specshow(Db_audio,sr=rate,x_axis="time",y_axis="hz")
Audio(audio_speech,rate=rate)

figure = plt.figure(figsize=(14,5))

audio_speech,rate = librosa.load(Main_Wav_Data["WAV"][4])

stft_audio = librosa.stft(audio_speech)
Db_audio = librosa.amplitude_to_db(abs(stft_audio))
librosa.display.specshow(Db_audio,sr=rate,x_axis="time",y_axis="hz")
Audio(audio_speech,rate=rate)

figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][2000])

stft_audio = librosa.stft(audio_speech)
Db_audio = librosa.amplitude_to_db(abs(stft_audio))
librosa.display.specshow(Db_audio,sr=rate,x_axis="time",y_axis="hz")
Audio(audio_speech,rate=rate)

Noise
figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][2000])

noise_injection = add_noise(audio_speech)

librosa.display.waveshow(noise_injection,sr=sample_rate)
Audio(noise_injection,rate=sample_rate)

Stretching
figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][2000])

stretching_audio = stretch_process(audio_speech)
librosa.display.waveshow(stretching_audio,sr=sample_rate)
Audio(stretching_audio,rate=sample_rate)

Shifting
figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][2000])

shifting_audio = shift_process(audio_speech)
librosa.display.waveshow(shifting_audio,sr=sample_rate)
Audio(shifting_audio,rate=sample_rate)

Pitch
figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][2000])

pitch_audio = pitch_process(audio_speech,sample_rate)
librosa.display.waveshow(pitch_audio,sr=sample_rate)
Audio(pitch_audio,rate=sample_rate)

Same Timeframe Period
figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][2000],duration=2.5,offset=0.4)
librosa.display.waveshow(audio_speech,sr=sample_rate)
print(audio_speech.shape)
Audio(audio_speech,rate=sample_rate)

figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][3],duration=2.5,offset=0.6)
librosa.display.waveshow(audio_speech,sr=sample_rate)
print(audio_speech.shape)
Audio(audio_speech,rate=sample_rate)

figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][1398],duration=2.5,offset=0.6)
librosa.display.waveshow(audio_speech,sr=sample_rate)
print(audio_speech.shape)
Audio(audio_speech,rate=sample_rate)


Data Processing and Engineering
Transformation and Exportation
x_Train, y_Train = [],[]

for path,emotion in zip(Main_Wav_Data.WAV,Main_Wav_Data.EMOTION):
    
    features = export_process(path)
    
    for element in features:
        x_Train.append(element)
        y_Train.append(emotion)

print(len(x_Train))
print(len(y_Train))
print(len(Main_Wav_Data.WAV))

print(x_Train[0].shape)

print(y_Train[0:5])

New_Features_Wav = pd.DataFrame(x_Train)
New_Features_Wav["EMOTIONS"] = y_Train

New_Features_Wav.to_csv("New_Wav_Set.csv",index=False)

New_Features_Wav.head(-1)
print(New_Features_Wav["EMOTIONS"].value_counts())

Splitting
#encoder_label = OneHotEncoder()
#encoder_label.fit(emotion_labels)

scaler_data = StandardScaler()

X = New_Features_Wav.iloc[:,:-1].values
Y = New_Features_Wav["EMOTIONS"].values

print(X.shape)
print(Y.shape)

if Y.ndim > 1:
    Y = np.argmax(Y,axis = 1)

encoder_label = LabelEncoder()
Y = encoder_label.fit_transform(Y)

print("Number of unique emotions:", len(np.unique(encoder_label.classes_)))
print("Unique emotion classes:", encoder_label.classes_)

with open("encoder_label.pkl", "wb") as f:
    pickle.dump(encoder_label,f)

print("LabelEncoder saved as encoder_label.pkl")

with open("encoder_label.pkl", "rb") as f:
    loaded_encoder = pickle.load(f)

print("Loaded object type:", type(loaded_encoder))
if hasattr(loaded_encoder, "classes_"):
    print("This is a valid LabelEncoder. Classes:", loaded_encoder.classes_)
else:
    print("Error: encoder_label.pkl is NOT a LabelEncoder!")

print(Y.shape)

xTrain,xTest,yTrain,yTest = train_test_split(X,Y,train_size=0.9,random_state=42,shuffle=True)

print(xTrain.shape)
print(yTrain.shape)
print(xTest.shape)
print(yTest.shape)

xTrain = scaler_data.fit_transform(xTrain)
xTest = scaler_data.transform(xTest)

with open("scaler_data.pkl", "wb") as f:
    pickle.dump(scaler_data,f)
print("Scaler saved as scaler_data.pkl")

print(xTrain.shape)
print(xTest.shape)

# Print detailed shape information
print("Original xTrain shape:", X.shape)
print("xTrain size before reshaping:", xTrain.size)
print("xTrain number of samples:", xTrain.shape[0])
print("xTrain number of features:", xTrain.shape[1] if len(xTrain.shape) > 1 else "N/A")

# Calculate expected reshape size
expected_reshape_size = xTrain.shape[0] * 704 * 1
print("Expected reshape size:", expected_reshape_size)

# If the sizes don't match, we'll need to investigate why
# Option 1: Trim or pad the data
def reshape_to_fixed_size(data, target_features=704):
    if data.shape[1] > target_features:
        # Trim excess features
        return data[:, :target_features]
    elif data.shape[1] < target_features:
        # Pad with zeros
        pad_width = ((0, 0), (0, target_features - data.shape[1]))
        return np.pad(data, pad_width, mode='constant')
    return data

# Preprocess xTrain and xTest
xTrain = reshape_to_fixed_size(xTrain)
xTest = reshape_to_fixed_size(xTest)


xTrain = xTrain.reshape(xTrain.shape[0], 704, 1)
xTest = xTest.reshape(xTest.shape[0], 704, 1)
print("Reshaped xTrain shape:", xTrain.shape)
print("Reshaped xTest shape:", xTest.shape)

yTrain = to_categorical(yTrain)
yTest = to_categorical(yTest)
print("Final yTrain shape:", yTrain.shape)
print("Final yTest shape:", yTest.shape)

print(xTrain.shape)
print(xTest.shape)


Model Structure
num_emotion_classes = len(np.unique(encoder_label.classes_))

Model=Sequential()
Model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(704, 1)))
Model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

Model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))
Model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

Model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))
Model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))
Model.add(Dropout(0.2))

Model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))
Model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

Model.add(Flatten())
Model.add(Dense(units=32, activation='relu'))
Model.add(Dropout(0.3))

Model.add(Dense(units=num_emotion_classes, activation='softmax'))

Model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])

Model.summary()

early_stop = tf.keras.callbacks.EarlyStopping(monitor="loss",patience=3,mode="min")

Conv1D_Model = Model.fit(xTrain, yTrain, batch_size=64, epochs=10, validation_data=(xTest, yTest), callbacks=[early_stop])

Model.save("audio_sentiment_model.keras")

Grap_Data = pd.DataFrame(Conv1D_Model.history)
figure = plt.figure(figsize=(10,10))
Grap_Data.plot()

plt.plot(Conv1D_Model.history["accuracy"])
plt.plot(Conv1D_Model.history["val_accuracy"])
plt.ylabel("ACCURACY")
plt.legend()
plt.show()

plt.plot(Conv1D_Model.history["loss"])
plt.plot(Conv1D_Model.history["val_loss"])
plt.ylabel("LOSS")
plt.legend()
plt.show()

Prediction
Model_Results = Model.evaluate(xTest,yTest)
print("LOSS:  " + "%.4f" % Model_Results[0])
print("ACCURACY:  " + "%.4f" % Model_Results[1])

prediction_test = Model.predict(xTest)
#y_prediction = encoder_label.inverse_transform(prediction_test)

#yTest = encoder_label.inverse_transform(yTest)

y_prediction_indices = np.argmax(prediction_test, axis=1)
#y_prediction = encoder_label.inverse_transform(y_prediction_indices)

yTest_indices = np.argmax(yTest, axis=1)
y_prediction = encoder_label.inverse_transform(y_prediction_indices)
yTest_labels = encoder_label.inverse_transform(yTest_indices)

conf_matrix = confusion_matrix(yTest_labels, y_prediction)

plt.figure(figsize=(10,8))
sns.heatmap(conf_matrix, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')

plt.title('Confusion Matrix', size=20)
plt.xlabel('Predicted Labels', size=14)
plt.ylabel('Actual Labels', size=14)
plt.show()

print(classification_report(yTest_labels, y_prediction))

print("Accuracy Score: ",accuracy_score(yTest_labels, y_prediction))

