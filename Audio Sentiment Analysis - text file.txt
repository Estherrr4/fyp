Packages and Libraries
#GENERAL
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import random

#PATH PROCESS
import os
import os.path
from pathlib import Path
import glob
from PIL import Image
!pip install keras tensorflow
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
!pip install opencv-python
import cv2
from keras.applications.vgg16 import preprocess_input, decode_predictions
import imageio
from IPython.display import Image
import matplotlib.image as mpimg

#MUSIC PROCESS
!pip install pydub
import pydub
from scipy.io.wavfile import read, write
!pip install librosa
import librosa
import librosa.display
import IPython
from IPython.display import Audio
import scipy
import pickle

#SCALER & TRANSFORMATION
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tensorflow.keras import regularizers
from sklearn.preprocessing import LabelEncoder

#ACCURACY CONTROL
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
#OPTIMIZER
from tensorflow.keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD

#MODEL LAYERS
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\
                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\
LSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape,\
Conv2DTranspose, LeakyReLU, Conv1D, AveragePooling1D, MaxPooling1D
from keras import models
from keras import layers
import tensorflow as tf
from keras.applications import VGG16,VGG19,inception_v3
from keras import backend as K
from keras.utils import plot_model
from keras.datasets import mnist
import keras

#SKLEARN CLASSIFIER
!pip install xgboost
from xgboost import XGBClassifier, XGBRegressor
!pip install lightgbm
from lightgbm import LGBMClassifier, LGBMRegressor
!pip install catboost
from catboost import CatBoostClassifier, CatBoostRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.cross_decomposition import PLSRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import RidgeCV
from sklearn.linear_model import Lasso
from sklearn.linear_model import LassoCV
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import ElasticNetCV

#IGNORING WARNINGS
from warnings import filterwarnings
filterwarnings("ignore",category=DeprecationWarning)
filterwarnings("ignore", category=FutureWarning) 
filterwarnings("ignore", category=UserWarning)


Path, Label, Tranformation
Main
Main_WAV_Path = Path("C:/Users/esther/OneDrive - Tunku Abdul Rahman University College/FYP/Project II/TESS Toronto emotional speech set data")

Wav Path
Wav_Path = list(Main_WAV_Path.glob(r"**/*.wav"))

Wav Labels
Wav_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Wav_Path))

To Series
Wav_Path_Series = pd.Series(Wav_Path,name="WAV").astype(str)
Wav_Labels_Series = pd.Series(Wav_Labels,name="EMOTION")

To Dataframe
Main_Wav_Data = pd.concat([Wav_Path_Series,Wav_Labels_Series],axis=1)
print(Main_Wav_Data.head(-1))
print(Main_Wav_Data["EMOTION"].value_counts())

To Shuffle
Main_Wav_Data = Main_Wav_Data.sample(frac=1).reset_index(drop=True)
print(Main_Wav_Data.head(-1))


Data Processing Function
Noise
def add_noise(data):
    noise_value = 0.015 * np.random.uniform() * np.amax(data)
    data = data + noise_value * np.random.normal(size=data.shape[0])
    
    return data

Stretch
def stretch_process(data,rate=0.8):
    
    return librosa.effects.time_stretch(y=data,rate=rate)

Shift
def shift_process(data):
    shift_range = int(np.random.uniform(low=-5,high=5) * 1000)
    
    return np.roll(data,shift_range)

Pitch
def pitch_process(data,sampling_rate,pitch_factor=0.7):
    
    return librosa.effects.pitch_shift(y=data,sr=sampling_rate,n_steps=pitch_factor)

Extract Features
def extract_process(data,sample_rate):
    output_result = np.array([])
    
    mean_zero = np.mean(librosa.feature.zero_crossing_rate(y=data).T,axis=0)
    output_result = np.hstack((output_result,mean_zero))
    
    stft_out = np.abs(librosa.stft(data))
    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft_out,sr=sample_rate).T,axis=0)
    output_result = np.hstack((output_result,chroma_stft))
    
    mfcc_out = np.mean(librosa.feature.mfcc(y=data,sr=sample_rate).T,axis=0)
    output_result = np.hstack((output_result,mfcc_out))
    
    root_mean_out = np.mean(librosa.feature.rms(y=data).T,axis=0)
    output_result = np.hstack((output_result,root_mean_out))
    
    mel_spectogram = np.mean(librosa.feature.melspectrogram(y=data,sr=sample_rate).T,axis=0)
    output_result = np.hstack([output_result, mel_spectogram])

    # if len(output_result) > 704:
        #output_result = output_result[:704]  # Trim excess features
    #elif len(output_result) < 704:
       # output_result = np.pad(output_result, (0, 704 - len(output_result)), mode='constant')
    
    return output_result

Export Features
def export_process(path):
    
    data,sample_rate = librosa.load(path,duration=2.5,offset=0.6)
    
    output_1 = extract_process(data,sample_rate)
    result = np.array(output_1)
    
    noise_out = add_noise(data)
    output_2 = extract_process(noise_out, sample_rate)
    result = np.vstack((result,output_2))
    
    new_out = stretch_process(data)
    strectch_pitch = pitch_process(new_out,sample_rate)
    output_3 = extract_process(strectch_pitch, sample_rate)
    result = np.vstack((result,output_3))
    
    return result


Analysis
Hearing
rate,speech = read(Main_Wav_Data["WAV"][2342])
print(Main_Wav_Data["EMOTION"][2342])

Audio(speech,rate=rate,autoplay=False)
rate,speech = read(Main_Wav_Data["WAV"][3])
print(Main_Wav_Data["EMOTION"][3])

Audio(speech,rate=rate,autoplay=False)

rate,speech = read(Main_Wav_Data["WAV"][2795])
print(Main_Wav_Data["EMOTION"][2795])

Audio(speech,rate=rate,autoplay=False)

Shape - Type
rate,speech = read(Main_Wav_Data["WAV"][2795])
print(Main_Wav_Data["EMOTION"][2795])
print(speech.shape)
print(speech.dtype)
print(rate)

rate,speech = read(Main_Wav_Data["WAV"][314])
print(Main_Wav_Data["EMOTION"][314])
print(speech.shape)
print(speech.dtype)
print(rate)

rate,speech = read(Main_Wav_Data["WAV"][134])
print(Main_Wav_Data["EMOTION"][134])
print(speech.shape)
print(speech.dtype)
print(rate)

Waveplot
figure = plt.figure(figsize=(14,5))

audio_speech,rate = librosa.load(Main_Wav_Data["WAV"][134])
librosa.display.waveshow(audio_speech,sr=rate)
Audio(audio_speech,rate=rate)

figure = plt.figure(figsize=(14,5))

audio_speech,rate = librosa.load(Main_Wav_Data["WAV"][34])
librosa.display.waveshow(audio_speech,sr=rate)
Audio(audio_speech,rate=rate)

figure = plt.figure(figsize=(14,5))

audio_speech,rate = librosa.load(Main_Wav_Data["WAV"][4])
librosa.display.waveshow(audio_speech,sr=rate)
Audio(audio_speech,rate=rate)

Specshow
figure = plt.figure(figsize=(14,5))

audio_speech,rate = librosa.load(Main_Wav_Data["WAV"][458])

stft_audio = librosa.stft(audio_speech)
Db_audio = librosa.amplitude_to_db(abs(stft_audio))
librosa.display.specshow(Db_audio,sr=rate,x_axis="time",y_axis="hz")
Audio(audio_speech,rate=rate)

figure = plt.figure(figsize=(14,5))

audio_speech,rate = librosa.load(Main_Wav_Data["WAV"][4])

stft_audio = librosa.stft(audio_speech)
Db_audio = librosa.amplitude_to_db(abs(stft_audio))
librosa.display.specshow(Db_audio,sr=rate,x_axis="time",y_axis="hz")
Audio(audio_speech,rate=rate)

figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][2000])

stft_audio = librosa.stft(audio_speech)
Db_audio = librosa.amplitude_to_db(abs(stft_audio))
librosa.display.specshow(Db_audio,sr=rate,x_axis="time",y_axis="hz")
Audio(audio_speech,rate=rate)

Noise
figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][2000])

noise_injection = add_noise(audio_speech)

librosa.display.waveshow(noise_injection,sr=sample_rate)
Audio(noise_injection,rate=sample_rate)

Streching
figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][2000])

stretching_audio = stretch_process(audio_speech)
librosa.display.waveshow(stretching_audio,sr=sample_rate)
Audio(stretching_audio,rate=sample_rate)

Shifting
figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][2000])

shifting_audio = shift_process(audio_speech)
librosa.display.waveshow(shifting_audio,sr=sample_rate)
Audio(shifting_audio,rate=sample_rate)

Pitch
figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][2000])

pitch_audio = pitch_process(audio_speech,sample_rate)
librosa.display.waveshow(pitch_audio,sr=sample_rate)
Audio(pitch_audio,rate=sample_rate)

Same Timeframe Period
figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][2000],duration=2.5,offset=0.4)
librosa.display.waveshow(audio_speech,sr=sample_rate)
print(audio_speech.shape)
Audio(audio_speech,rate=sample_rate)

figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][3],duration=2.5,offset=0.6)
librosa.display.waveshow(audio_speech,sr=sample_rate)
print(audio_speech.shape)
Audio(audio_speech,rate=sample_rate)

figure = plt.figure(figsize=(14,5))

audio_speech,sample_rate = librosa.load(Main_Wav_Data["WAV"][1398],duration=2.5,offset=0.6)
librosa.display.waveshow(audio_speech,sr=sample_rate)
print(audio_speech.shape)
Audio(audio_speech,rate=sample_rate)

Data Processing and Engineering
Transformation and Exportation
x_Train, y_Train = [],[]

for path,emotion in zip(Main_Wav_Data.WAV,Main_Wav_Data.EMOTION):
    
    features = export_process(path)
    
    for element in features:
        x_Train.append(element)
        y_Train.append(emotion)

print(len(x_Train))
print(len(y_Train))
print(len(Main_Wav_Data.WAV))

print(x_Train[0].shape)
print(y_Train[0:5])

New_Features_Wav = pd.DataFrame(x_Train)
New_Features_Wav["EMOTIONS"] = y_Train

New_Features_Wav.to_csv("New_Wav_Set.csv",index=False)

New_Features_Wav.head(-1)

print(New_Features_Wav["EMOTIONS"].value_counts())

Splitting
#encoder_label = OneHotEncoder()
#encoder_label.fit(emotion_labels)

scaler_data = StandardScaler()

X = New_Features_Wav.iloc[:,:-1].values
Y = New_Features_Wav["EMOTIONS"].values

print(X.shape)
print(Y.shape)

if Y.ndim > 1:
    Y = np.argmax(Y,axis = 1)

encoder_label = LabelEncoder()
Y = encoder_label.fit_transform(Y)

with open("encoder_label.pkl", "wb") as f:
    pickle.dump(encoder_label,f)

print("LabelEncoder saved as encoder_label.pkl")

with open("encoder_label.pkl", "rb") as f:
    loaded_encoder = pickle.load(f)

print("Loaded object type:", type(loaded_encoder))
if hasattr(loaded_encoder, "classes_"):
    print("This is a valid LabelEncoder. Classes:", loaded_encoder.classes_)
else:
    print("Error: encoder_label.pkl is NOT a LabelEncoder!")

print(Y.shape)

xTrain,xTest,yTrain,yTest = train_test_split(X,Y,train_size=0.9,random_state=42,shuffle=True)

print(xTrain.shape)
print(yTrain.shape)
print(xTest.shape)
print(yTest.shape)

xTrain = scaler_data.fit_transform(xTrain)
xTest = scaler_data.transform(xTest)

print(xTrain.shape)
print(xTest.shape)

#xTrain = np.expand_dims(xTrain,axis=2)
#xTest = np.expand_dims(xTest,axis=2)
xTrain = xTrain.reshape(xTrain.shape[0],704, 1)
xTest = np.expand_dims(xTest.shape[0], 704, 1)

yTrain = to_categorical(yTrain)
yTest = to_categorical(yTest)

print(xTrain.shape)
print(xTest.shape)


Model Structure
Model=Sequential()
#Model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(xTrain.shape[1], 1)))
Model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(704, 1)))
Model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

Model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))
Model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

Model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))
Model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))
Model.add(Dropout(0.2))

Model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))
Model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

Model.add(Flatten())
Model.add(Dense(units=32, activation='relu'))
Model.add(Dropout(0.3))

Model.add(Dense(units=14, activation='softmax'))

Model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])

early_stop = tf.keras.callbacks.EarlyStopping(monitor="loss",patience=3,mode="min")

Conv1D_Model = Model.fit(xTrain, yTrain, batch_size=64, epochs=10, validation_data=(xTest, yTest), callbacks=[early_stop])

Model.save("audio_sentiment_model.keras")

Grap_Data = pd.DataFrame(Conv1D_Model.history)
figure = plt.figure(figsize=(10,10))
Grap_Data.plot()

plt.plot(Conv1D_Model.history["accuracy"])
plt.plot(Conv1D_Model.history["val_accuracy"])
plt.ylabel("ACCURACY")
plt.legend()
plt.show()

plt.plot(Conv1D_Model.history["loss"])
plt.plot(Conv1D_Model.history["val_loss"])
plt.ylabel("LOSS")
plt.legend()
plt.show()

Prediction
Model_Results = Model.evaluate(xTest,yTest)
print("LOSS:  " + "%.4f" % Model_Results[0])
print("ACCURACY:  " + "%.4f" % Model_Results[1])

prediction_test = Model.predict(xTest)
y_prediction = encoder_label.inverse_transform(prediction_test)

yTest = encoder_label.inverse_transform(yTest)

print(prediction_test[0:10])

print(y_prediction[0:10])

print(yTest[0:10])

conf_matrix = confusion_matrix(yTest, y_prediction)

sns.heatmap(conf_matrix, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')

plt.title('Confusion Matrix', size=20)
plt.xlabel('Predicted Labels', size=14)
plt.ylabel('Actual Labels', size=14)
plt.show()

print(classification_report(yTest, y_prediction))

print(accuracy_score(yTest, y_prediction))

import os
import tensorflow as tf

# Set project path (modify if needed)
project_path = "C:/Users/esther/OneDrive - Tunku Abdul Rahman University College/FYP/Project II"
os.chdir(project_path)

# Load pre-trained model
model_path = os.path.join(os.getcwd(), "audio_sentiment_model.keras")

if os.path.exists(model_path):
    model = tf.keras.models.load_model(model_path)
    print("âœ… Model loaded successfully!")
    print("ğŸ” Model expected input shape:", model.input_shape)
else:
    print(f"âŒ Error: Model file '{model_path}' not found!")

import pickle
from sklearn.preprocessing import LabelEncoder

# Load label encoder
encoder_path = os.path.join(os.getcwd(), "encoder_label.pkl")

if os.path.exists(encoder_path):
    with open(encoder_path, "rb") as f:
        encoder_label = pickle.load(f)
    
    print("âœ… Label encoder loaded successfully!")
    print("ğŸ” Label encoder type:", type(encoder_label))
    
    # Verify if it's a LabelEncoder
    if isinstance(encoder_label, LabelEncoder):
        print("Classes in LabelEncoder:", encoder_label.classes_)
    else:
        print("âš ï¸ Error: encoder_label.pkl is NOT a LabelEncoder!")
else:
    print("âŒ Error: Label encoder file not found!")

import librosa
import numpy as np

def extract_features(audio_path):
    num_mfcc = 40
    num_chroma = 12
    num_mel = 128
    
    data, sample_rate = librosa.load(audio_path, duration=2.5, offset=0.6)

    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=num_mfcc).T, axis=0)
    chroma = np.mean(librosa.feature.chroma_stft(y=data, sr=sample_rate).T, axis=0)
    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)
    
    features = np.hstack([mfcc, chroma, mel])
    print("Extracted Features Shape BEFORE reshaping:", features.shape)

    expected_shape = 704
    if features.shape[0] > expected_shape:
        features = features[:expected_shape]  # Trim excess
    elif features.shape[0] < expected_shape:
        features = np.pad(features, (0, expected_shape - features.shape[0]), mode='constant')

    print("Extracted Features Shape AFTER reshaping:", features.shape)
    
    return features.reshape(1, 704)

# Test with a sample audio file
audio_path = "C:/Users/esther/OneDrive/Document/YYQX/2025 Feb Audio.wav"  # Change this to your test file
features = extract_features(audio_path)

# Ensure features are correctly shaped
print("Feature Shape for Prediction:", features.shape)

# Make prediction
#features = features.reshape(1,704)
#prediction = model.predict(features)
#print("Raw Prediction Output:", prediction)

# Get predicted label index
#predicted_label_index = np.argmax(prediction)
#print("Predicted Label Index:", predicted_label_index)

# Decode sentiment using label encoder
#if isinstance(encoder_label, LabelEncoder):
 #   sentiment = encoder_label.inverse_transform([predicted_label_index])[0]
 #   print("Predicted Sentiment:", sentiment)
#else:
    #print("âš ï¸ Error: Label encoder is not a valid LabelEncoder!")
